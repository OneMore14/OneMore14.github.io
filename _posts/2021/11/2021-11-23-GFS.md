
layout: post
title:  "GFS 论文阅读"
date:   2021-11-23 20:34:01 +0800
categories: 6.824



# The Google File System



### 1.  INTRODUCTION

GFS和其他分布式文件系统一样，有着对性能、扩展性、可用性等方面的追求，但结合实际看，还是有一些不同以往的设计思想。

首先，component failures更常见，这些错误可能是程序bug，操作系统bug，人为错误，硬件或网络错误等各种情况。因此，持续监控、错误检测、fault tolerance和自动恢复必须集成在系统中。其次，文件越来越大，GB级文件很常见，需要考虑处理大文件的IO和重新设定block size。第三，大多数文件的修改方式是追加在文件末尾而不是重写覆盖，随机写文件在实践中是并不常见，因此cache的作用不高。第四，同时设计考虑应用和文件系统API，有助于提升整体系统的灵活性。

### 2. DESIGN OVERVIEW

### 2.1 Assumptions

* 系统运行在不贵的普通机器上，因此经常fail，系统必须保证持续监控、检测错误、恢复等功能
* 系统存储大量较大文件，比如几百万个文件，每个文件100M以上。Gb级大文件也常见且需要高效管理。GFS也支持小文件，但不对小文件做优化。
* 负载主要由两种读组成，large streaming reads 和 small random reads。前者一般连续读几百K或者1M，后者则是在任意位置读取几Kb。Performance-conscious applications often batch and sort their small reads to advance steadily through the file rather than go back and forth.
* 负载也包括大量顺序写，通常追加在文件尾部，写入后的文件一般不会再改变。同样也有一些在任意位置的小量写入，系统可以支持但不需要考虑优化
* 系统需要高效实现多个客户端同时追加写入同一个文件的场景。
* 高可持续带宽比低延时更重要。

### 2.2 Interface

提供常见的文件系统接口，且按层次组织，以路径名为标志。 GFS提供snapshot和record append两种操作，snapshot可以以小代价创建文件或目录树的副本，record append支持多个客户端同时向一个文件追加写，GFS保证了每个独立record append操作的原子性，这有助于用户实现multi-way merge和生产者-消费者队列，用户不需要提供额外的锁

### 2.3 Architecture

![image-20211124095901722](/assets/2021/11/gfs/image-20211124095901722.png)

GFS有一个master和多个chunkserver，被多个client访问。这3种角色一般都以用户进程的身份运行在普通的Linux机器上。chunkserver还可以和client运行在同一台机器上。

文件被分割成固定大小的chunk，每个chunk在创建时被master分配一个不可改变且唯一的64位 chunk handle。chunkserver像普通Linux文件一样将chunk保存在本地磁盘，读取时根据chunk handle和byte range确定读取的数据。为保证可靠性，每个chunk会被保存在不同的机器上，一般是保存3份，该值用户可自行设置。

master管理文件系统所有元数据，master和server定时心跳检测。

client和master交流文件元数据的信息，和chunkserver直接交流文件本身的内容。

client和chunkserver都不缓存文件内容，大多数client应用流式读取大文件内容或者需要工作的文件范围太大，导致cache不实用，但client是会缓存文件的元数据。chunkserver不需要缓存是因为chunk保存在本地磁盘，Linux已经为chunk提供了一层缓存。

### 2.4 Single Master

单一master能大大简化实现，但必须简化其读写操作的演进以防止master成为瓶颈。client读写文件并不经过master，client只向master查询其应该与哪一个chunkserver交互，然后cache这个信息，利用这个信息直接与chunkserver交互。

client需要读数据时，先根据固定的chunk size算出chunk index，然后将filename和chunk index发送给master，master返回filename对应的chunk handle和chunk所在chunkserver对应的chunk location，然后client向chunk location对应的chunkserver发起读取请求，chunkserver返回数据。

### 2.5 Chunk Size

作者选择64MB作为大小，比普通文件系统的block size大很多，有一些重要的好处。首先，大大减少client和master的查询交互。其次，大chunk size保证client可以在一个client上做很多操作，可以维持一个TCP长连接减少网络开销。第三，减少master需要维护的元数据，使元数据可以保存在内存中

但另一方面，大chunk size也有坏处，对于小文件，可能只有一个chunk大小，当多个client同时访问时，对应的chunkserver会成为热点(hot spot)。当然，在实际应用中，hot spot并不是主要的问题，因为应用大多是顺序读取大文件的。但在GFS早期确实遇到过这个问题，当时解决方案是调大副本数

### 2.6 Metadata

master存储三类元数据

* the file and chunk namespace，保存在内存和磁盘的operation log中
* files到chunks的映射，同上
* chunk每一个副本的位置，只保存在内存中，每当master启动或有新的chunkserver加入系统时更新。

#### 2.6.1 In-Memory Data Structures

既然元数据都保存在内存，master的操作是很快的，因此master可以周期性扫描所有元数据，扫描的作用是实现chunk 垃圾回收，为宕机的chunk重新创造副本或者移动chunk以达到负载均衡。不用担心系统整体的大小被master的内存所限制，一个64MB大小的chunk，其元数据不到64bytes，且为master提供大内存是很廉价的升级。

#### 2.6.2 Chunk Locations

chunk location只保存在master内存中，而不是持久化到磁盘。作者团队期初也尝试将位置信息保存在master磁盘中，但这样会遇到大问题，每当chunkserver加入、重启、宕机、恢复等情况发生时，需要同步本地数据。另一个视角看这种设计在于，chunkserver才应当真正决定chunk位置是否有效，而不是master本地的一个文件。

#### 2.6.3 Operation Log

Operation log是master唯一持久化的元数据，因此非常重要，其不仅是对元数据的保存，更是提供了各个操作的时间线。Operation log需要可靠存储，在其他远程机器上有副本，且只有在写入到本地和远程磁盘后才会响应client请求。

master可以通过回放操作日志来恢复文件系统状态，为了减少master的启动时间，要尽量让Operation log小。master会在log增长到一定大小后创建一个checkpoint。master可以直接从checkpoint恢复。

### 2.7 Consistency Model

GFS拥有较为宽松的一致性模型

#### 2.7.1 Guarantees by GFS

文件命名空间修改(比如创建文件)是原子的，处理它们时需要加上全局锁, 加锁修改也生成了操作的timeline。

![image-20211124160026680](/assets/2021/11/gfs/image-20211124160026680.png)

文件区域(file region)在一次数据修改后的状态取决于修改的类型，一个文件区域被认为是consistent，如果所有客户端读到的都是一样的数据，这里并不关心是否都读的是同一个副本。一个区域是$defined$​，如果在一次修改后其状态是consistent，且client可以看到修改后的内容。当一次没有并发影响的写入成功后，被写入的区域就是$defined$​。并发的成功写入导致对应区域是$undefined\ but\ consistent$​​​​​​，所有client将看到一样的数据，但内容可能并不是任何一次修改后的内容，一般来说，其包含的是多次修改的混合片段。一次失败的修改让region变成inconsistent，不同client可能在不同时间看见不同的数据。GFS会区分出defined和undefined region，但不会区分不同undefined region的类别

Data mutation分为两种，writes或record appends。一次write是由应用决定写入到特定位置，一次record append将使data被原子性地追加至少一次，即使是在同时被追加，record append的写入位置offset是由GFS系统决定的(相比较而言，对client来说，普通文件系统上的追加无非是在当前文件末尾的位置写入)。GFS会将offset返回给client，并标记defined region的起始位置.

在一系列成功的修改后，GFS保证被修改的文件是defined的，且包含最后一次修改的内容。GFS达成这种效果的方法是:

1. 对所有chunk的副本，保证各个操作的顺序相同
2. 使用chunk version number检测stale chunk。stale chunk不会再被修改，也不会允许client读取，它们将被master垃圾回收

master不仅和chunkserver周期性做心跳检测，也会对chunkserver保存的数据做校验和。一个chunk只有在所有副本都失效时才会不可逆地失效。

#### 2.7.2 Implications for Applications



### 3. SYSTEM INTERACTIONS

### 3.1 Leases and Mutation Order

对chunk的修改需要应用到每一个副本上，作者使用租约来维护各个副本在修改操作上的一致性。master给某个副本以租约，该副本变成primary chunk。primary chunk对所有修改确定顺序，其他副本也遵照这个顺序。租约是为了尽量减少master的管理开销，租约初始时只有60秒，但只要chunk还在被修改，primary chunk就可以无限续租。租约的信息搭载在master和chunk server之间的心跳检测信息上。master可能会在租约到期前收回租约。即使master和primary chunk之间断了联系，其也能在旧租约到期后重新给其他副本分一个租约



![image-20211128174622579](/assets/2021/11/gfs/image-20211128174622579.png)



上图即为控制写入的步骤

1. client询问master哪一个chunkserver有指定chunk的租约和其他副本的位置，如果都没有租约，master会自己选择一个
2. master返回第一步查询的结果，client将返回结果缓存，client只有在primary不可达或者primary回应自己没有租约时，才会重新向master查询
3. client推送数据给所有副本，推送顺序任意。每个chunkserver将数据存储在内部LRU buffer中直到数据被使用或到期。解耦数据流和控制流，可以提高性能，因为数据流可以根据网络结构来调度。
4. 当所有副本确认已经收到数据后，client向master发起写请求，写请求指明了数据是之前推的某一个数据，primary给其接收的所有操作按顺序编号，并按顺序执行修改。
5. primary转发写请求到其他副本，每个副本也按primary的编号顺序执行操作
6. 所有副本向primary确认已经完成操作
7. primary向client回复，在任意副本的任意错误，都会报告给client。如果有错误，client request被认为失败了，且修改的区域出现了不一致，client需要重复几次3~7步，如果仍然失败，可能需要退回到第一步

如果client要写的数据太大或者跨过了两个chunk的边界，client会把数据和请求分为几次，虽然最终各个副本可以保持一致，但分割请求之间，一个chunk可能有另一个client在修改，造成数据混在一起。这种是consistent but undefined file region

### 3.2 Data Flow

为了充分利用每台机器的网络带宽，避免网络瓶颈和高延迟链接，调度数据沿着选择的chunkserver链流式传输。每台机器转发数据给网络结构中最近的机器。比如client要给几个chunkserver发数据，可以先发给最近的chunkserver1，然后chunkserver1就将数据再转发给chunkserver2/3/4中离它最近的，以此类推。机器间的距离可以简单用IP地址来衡量。

为了减少latency，传输采用流水线方式。一个chunkserver收到部分数据后就开始转发。

### 3.3 Atomic Record Appends

record append时，client只能决定数据，不能决定写入的位置，写入位置由GFS决定，并将位置返回给client。record append写入逻辑和3.1节中差不多，区别是在primary。client还是将数据推给每个副本后向primary发起请求，primary检查追加数据后chunk大小是否会超过最大值。如果会超过，先将当前chunk填充到最大值，并命令副本也这样做，然后回复让client在下一个chunk重试。如果没超，就追加写，告诉其他副本写入的具体位置，再返回位置给client。

如果record append在任一副本失败，client重试该操作。结果，副本们包含的record数据可能不一样，GFS不保证所有副本完全相同，只保证数据至少被原子性写入一次

### 3.4 Snapshot

创建snapshot使用copy-on-write，当master接收快照请求后，首先收回所有租约。之后记录操作到硬盘，让快照指向chunkserver的对应位置。如果后续需要修改chunk，则master先让chunkserver创造出真正的副本，然后让client修改这个新造出的副本。

### 4. MASTER OPERATION

### 4.1 Namespace Management and Locking

有些master操作(比如snapshot)比较耗时，因此master需要支持多个操作，为此在namespace引入锁。GFS的namespace不是像传统的那种文件夹形式，而是用于从路径名映射到元数据。GFS为每一个文件/文件夹名配一把读写锁。

每个master操作都需要先获取到锁，例如要访问/写入 /d1/d2/../dn/leaf，要先获取/d1, /d1/d2, ..., /d1/d2/../dn的读锁，和最后/d1/d2/../dn/leaf的读锁或写锁，这里leaf可以是文件或文件夹。注意，创建一个文件时，比如/d1/foo，并不需要/d1的写锁，只需要读锁，因为并没有真正的"目录"关系。这样也可以支持一个目录下同时修改多个文件

### 4.2 Replica Placement

### 4.3 Creation, Re-replication, Rebalancing

创造chunk只有3种原因 Creation, Re-replication, Rebalancing

当副本数不足时，master立刻重新复制副本

### 4.4 Garbage Collection

文件被删除后，GFS并不立即释放空间，而是通过渐进的垃圾回收过程，这样能提高系统的可靠性和使系统更简洁。

### 4.4.1 Mechanism

文件被删除后，master立即将此信息写入log，然而并不立即宣称释放空间而是将文件名改为包含删除时间的hidden name。master会日常扫描这些隐藏文件，并删除超过3天的文件。在此之前，文件都可以被恢复。隐藏文件被删除时，master内的元数据也被删除。master也会扫描孤块并删除，在和chunkserver的心跳检测中，chunkserver报告一些chunk信息，master会返回哪些是孤块，chunkserver可以将它们删除。

### 4.4.2 Discussion

对每一个chunk，有file-to-chunk表可以判断是否为孤块。这种靠垃圾回收实现删除的主要坏处是，如果硬盘存储很紧张时比较麻烦。

### 4.5 Stale Replica Detection

一个chunk可能是stale的如果对它修改失败。master为每一个chunk维护了chunk version number。每当master为chunk分配一个租约时，就将其版本号增加，master和所有副本全部记录最新的版本号。master也会把版本号告诉client。

### 5. FAULT TOLERANCE AND DIAGNOSIS

### 5.1 High Availability

GFS保证系统整体高可用的两种简单但有效的方法 fast recovery and replication

#### 5.1.1 Fast Recovery

#### 5.1.2 Chunk Replication

#### 5.1.3 Master Replication

master的Operation log和checkpoint会备份到多台机器。一次修改只有其操作日志被写入磁盘且同步到其他机器后才算提交。如果master的硬盘或网络故障，GFS的外部监控程序可以立即在其他机器起一个master进程，

### 5.2 Data Integrity

每个chunkserver使用校验和去检测存储数据损坏。一个chunk被分成最多64KB blocks，每一个block有32bit校验和，校验和存在内存和持久化日志里。chunkserver在回复读请求时，如果发现校验不对，会回复错误并报告给master，请求者会向其他chunkserver请求数据，master会给chunkserver复制一个其他副本。

