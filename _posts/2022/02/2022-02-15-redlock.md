---
layout: post
title:  "Redlock discussion"
date:   2022-02-15 21:25:01 +0800
categories: "分布式 Redis"
typora-root-url: "../../../"
---

关于Redis实现分布式锁的讨论

# 1. Distributed locks with Redis

*(注: 原文链接[在此](https://redis.io/topics/distlock))*

本文将提出一种新的算法 **Redlock**

#### Safety and Liveness guarantees

在我们看来，高效实现分布式锁至少需要保证以下3个性质

* **Safety property**: Mutual exclusion. At any given moment, only one client can hold a lock.
* **Liveness property A**: Deadlock free. Eventually it is always possible to acquire a lock, even if the client that locked a resource crashes or gets partitioned.
* **Liveness property B**: Fault tolerance. As long as the majority of Redis nodes are up, clients are able to acquire and release locks.

#### Why failover-based implementations are not enough

​        为了理解本文提出的改进，先分析下当前大多数Redis分布式锁的情况。

​        最简单方法是获取锁时创建一个key，且给key设置过期时间，释放锁时删除key。

​        表面上看不错，但有一个问题: 无法应对单点故障。如果使用主从模型，master挂掉后使用slave呢？这仍然不可靠，因为Redis复本更新是异步的

​        在这个模型中有明显的竞争条件:

1. Client A acquires the lock in the master.
2. The master crashes before the write to the key is transmitted to the replica.
3. The replica gets promoted to master.
4. Client B acquires the lock to the same resource A already holds a lock for. **SAFETY VIOLATION!**

​        在某些特殊情况，这种多个客户端能同时拿到锁也许是OK的，那么可以使用上述算法，否则推荐使用下面的算法。

#### Correct implementation with a single instance

​        先看简单的单机情况，加锁时设置

```
 SET resource_name my_random_value NX PX 30000
```

​        NX使只有key不存在时才能成功，PX使key30000毫秒后失效。设置的value必须保证全局唯一(across all clients and all lock requests)

​        一般使用随机的值保证释放锁的安全性，使用以下Lua脚本

```lua
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

​        主要是防止这样一种情况: Client A先加锁，执行了过长的计算过程，导致A设置的key已经过期且B获得锁，当A结束计算准备释放锁时，不能误把B的锁删了*(注: 类似CAS的思想)*。

​        随机值的设定有很多种方法，比如当前时间戳带上client id。目前的算法在单机具有always available的保证下已经安全了，然后会在没有保证的情况下讨论分布式锁。设置的过期时间长度也叫lock validity time或auto release time。

#### The Redlock algorithm

​      在分布式环境下，假设我们有N台Redis master，这些节点完全独立，可能宕机。我们不使用副本机制或其它隐式的协调系统。本文中N = 5，客户端获取锁的算法如下

1. It gets the current time in milliseconds.
2. It tries to acquire the lock in all the N instances sequentially, using the same key name and random value in all the instances. During step 2, when setting the lock in each instance, the client uses a timeout which is small compared to the total lock auto-release time in order to acquire it. For example if the auto-release time is 10 seconds, the timeout could be in the ~ 5-50 milliseconds range. This prevents the client from remaining blocked for a long time trying to talk with a Redis node which is down: if an instance is not available, we should try to talk with the next instance ASAP.
3. The client computes how much time elapsed in order to acquire the lock, by subtracting from the current time the timestamp obtained in step 1. If and only if the client was able to acquire the lock in the majority of the instances (at least 3), and the total time elapsed to acquire the lock is less than lock validity time, the lock is considered to be acquired.
4. If the lock was acquired, its validity time is considered to be the initial validity time minus the time elapsed, as computed in step 3.
5. If the client failed to acquire the lock for some reason (either it was not able to lock N/2+1 instances or the validity time is negative), it will try to unlock all the instances (even the instances it believed it was not able to lock).

*(注: 在单机的基础上，要求获得过半数节点的支持才获取锁成功。且在获取锁的时候，考虑到了访问每个Redis实例共花费的时间，一个优化是在访问每个Redis实例时设置timeout)*

####  Is the algorithm asynchronous?

​        这个算法需要满足一个前提: 当进程间没有同步时钟时，每个进程的本地时间是差不多的，至少时间误差和锁的auto-release time相比很小。这个假设在现实世界中也是基本成立的。此时，我们需要重新更准确定义client持有锁的时间，应当是lock validity time减去为了获得锁花费的时间

#### Retry on failure

​        当客户端无法拿到锁时，其应该在随机延迟后重试，随机是为了防止产生脑裂而没有winner产生。同时，客户端拿到超半数成功的时间越短，产生脑裂的可能就越小，因此理想情况下，客户端应当使用multiplexing同时向Redis实例发消息。

​        同时要强调，当client无法拿到超半数锁时，应当尽快把已经拿到的锁释放。

####  Releasing the lock

​        释放锁就是在所有实例上释放获得的锁

#### Safety arguments

​        首先讨论一个客户端能获取到超半数锁的情况。假设此时所有实例都设置成功，有相同的TTL，但不同实例设置成功的时间并不相同，因此各个实例实际key过期的时间有差异。假设第一个key最晚在T1时间设置成功，最后一个key最晚在T2时间设置成功，则所有实例key都存在的时长是 `MIN_VALIDITY=TTL-(T2-T1)-CLOCK_DRIFT`   

​        当超半数key都设置成功后，如果再来一个client尝试获取分布式锁，一定会获取失败。

​        但我们还想要保证多个client同时获取锁时，不能同时成功。如果一个client获取到了超半数锁，但消耗的时间邻近或超过了maximum validity time，那么client会认为获取锁失败然后释放锁。因此我们只需要考虑耗时小于validity time的情况。根据上文可知，MIN_VALIDITY时间段内，是不可能获取超半数锁的。所以多个客户端同时获取锁的情况只能是一个client获取到锁后发现已经过了TTL，这个client会再释放锁。不过这里的安全性还需要形式化证明。

#### Liveness arguments

​        系统的liveness基于以下3个特点

1. The auto release of the lock (since keys expire): eventually keys are available again to be locked.
2. The fact that clients, usually, will cooperate removing the locks when the lock was not acquired, or when the lock was acquired and the work terminated, making it likely that we don’t have to wait for keys to expire to re-acquire the lock.
3. The fact that when a client needs to retry a lock, it waits a time which is comparably greater than the time needed to acquire the majority of locks, in order to probabilistically make split brain conditions during resource contention unlikely.

#### Performance, crash-recovery and fsync

​        为了保证高性能，可以使用multiplexing同时发请求。

​         同时考虑容错能力，假设我们配置的Redis实例没有任何持久化机制，当前client获取到了5个锁中的3个。然后这3台实例中有一台出错后重启，重启后就丢失了之前的key，此时另一个client可以再获得3个锁，造成不一致。

​        一种办法是开启AOF持久化，但持久化一般是异步写数据到磁盘的，比如每秒一次，那么仍然可能key在被持久化前实例就重启了。如果开启同步持久化，则Redis性能会大打折扣。

​        另一种解决办法是，几乎不用考虑持久化，而是让重启后的实例等待一段时间后再加入分配锁的群体中。等待时间需要超过最大的TTL。不过当超半数机器重启时，会对整个系统带来TTL的延迟。         

#### Making the algorithm more reliable: Extending the lock

​        如果client的任务由几个小步骤组成，可以默认使用较短的 lock validity time，当快到过期时间时，client向所有实例发送一个Lua脚本尝试延长锁的时间。延长锁仍然需要得到超半数支持和在 validity time内完成。延长锁的机制并没有改变算法，因此需要限制重新获取锁的尝试次数，否则会破坏liveness性质





# 2. How to do distributed locking

*(注: 原文链接[在此](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html))*

​        Redis本身很适合transient, approximate, fast-changing data between servers，而且能容忍偶尔数据丢失的场景。但Redis逐渐进入对数据一致性和持久性有要求的领域，这本身并不是Redis的设计目标之一。

## What are you using that lock for?

​        从高层视角看，在分布式应用中需要锁的原因有两个: efficiency或者correctness。

* **Efficiency**: 使用锁可以防止同样的任务被多次执行，尤其是一些运算量大的任务。如果锁失败，两个节点做了完全一样的工作，那么整个资源的消耗就加倍。
* **Correctness**: 保证运算结果正确

​        两种情况都可能需要锁，我们需要明确自己到底需要哪一种。 

​        如果使用锁只是为了efficiency，那么不需要引入Redlock运行5个server和检查超半数实例的开销和复杂度。最好只使用一个Redis实例，或者再开启异步同步到其他节点以防master崩溃。

​        如果只使用一个Redis实例，不可避免地会有Redis实例出现故障的情况，但因为此时锁的用途只是为了Efficiency，所以偶尔的故障不是大问题，这本身也是Redis自己的特点。因此这种锁的使用方法可以用在不是非常关键的场景。

​        另一方面，Redlock算法，靠5个副本和大多数投票，乍一看很适合追求correctness的场景，但实际可能并不适合。下文将详细讨论 

## Protecting a resource with a lock

​        先不管Redlock本身，首先讨论分布式锁通常是如何使用的。需要明确的是分布式锁和多线程中的mutex不同，要更复杂，因为不同节点可能会因为各种原因故障，网络也可能各种原因故障。

​        例如，假设有一个应用，client需要更新存储。client获取锁 -> 读文件 -> 修改文件 -> 写回修改后的文件 -> 释放锁。在这个场景中，锁阻止两个client同时修改文件，可能造成更新丢失(lost updates)。代码可能如下:

```java
// THIS CODE IS BROKEN
function writeData(filename, data) {
    var lock = lockService.acquireLock(filename);
    if (!lock) {
        throw 'Failed to acquire lock';
    }

    try {
        var file = storage.readFile(filename);
        var updated = updateContents(file, data);
        storage.writeFile(filename, updated);
    } finally {
        lock.release();
    }
}
```

​        不幸的是，即使lock service完全正确，代码也是错误的，下面的图片示例



![image-20220305203638145](/assets/2022/02/redlock/image-1.png)

​        Client1拿到锁后遇到GC，可能会暂停几分钟，导致运算超时，在自己没有写回数据前将锁释放，最终造成数据不一致。HBase曾经遇到过这个真实问题。

​        也许会想到在准备写回的时候再次检查，但GC是可能发生在任意时间的，因此引入检查是不可行的。

​        也许会想到自己用的编程语言没有那么长的GC，但可能有其他事件导致进程中断，比如page fault，比如等待网络数据，比如CPU有很多进程需要调度，或者某人给当前进程发送了SIGSTOP信号。总之，进程很可能被暂停。

​        这就是上面代码出问题的原因，lock service再好也没有用。

## Making the lock safe with fencing

​        一个简单的解决办法是在每次写回文件时引入 fencing token。在当前场景中，fencing token可以是递增的数字(由 lock服务提供)，每次获取锁时，同时获得一个数字作为fencing token

![image-20220305211111643](/assets/2022/02/redlock/image-2.png)

​        fencing token由lock服务提供，比如假设ZooKeeper 是lock服务，那么可以返回zxid或者znode version number作为fencing token。

​        到此，引入了一个Redlock的问题: 没有办法生成fencing token。Redlock算法没有在每次获取锁时产生任何编号。这意味着即使Redlock没有问题，也是不安全的，因为可能存在一个client进程被暂停，即上一节讲的问题。

​        如果要修改Redlock算法来产生生成fencing token也是不容易的。Redlock使用的随机值是不保证单调性的。如果简单地在Redis实例中维护一个counter也是不可靠的，因为实例可能会出故障，如果再引入多个实例来容错则要求节点间信息的同步和保证数据一致性， 则需要再引入共识算法，整个系统非常复杂。

## Using time to solve consensus

​        Redlock很难生成fencing token这一点已经足够说明其不适用于correctness的场景。但还有其他一些问题值得讨论。

​        在学术领域，这类问题最现实的模型是[asynchronous model with unreliable failure detectors](http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf). 简单说就是该算法不对任何timing作假设: 进程可能暂停任意长时间，数据包可能在网络中延迟任意时间，并且时钟也可能任意错误，算法也不预期做正确的事。

​        算法中唯一需要使用时钟的是生成timeout，是为了避免无限等待。但timeout不需要完全准确，因为一个请求timeout可能是数据包在网络中有延迟，也可能是本地时钟出错。总之，timeout作为failure detector，只是猜测某个环节出现问题。

​        注意Redis使用的是gettimeofday来决定key是否过期，这样时钟完全可能突然向前或向后(由管理员手动调整或NTP服务器调整)，那么key在Redis中实际的存在时间可能和预期非常不同。

​        对于异步模型中的算法，这些不是大问题: 这些算法不需要任何timing假设就可以保证safety性质永远满足。只有liveness性质需要依赖timeout或其他错误检测。简单说，系统timing出现任何问题(进程暂停、网络延迟、时钟跳转)等都只影响算法的效率，而不会让算法做出错误的决定。

​        然而，Redlock并不是这种模型，其safety性质需要timing假设: 假设了Redis节点几乎正确地维护了key的存在时间、网络延迟相比失效时间更小且进程暂停时间远比失效时间短。

## Breaking Redlock with bad timings

​        看一些具体的例子，假设有5个Redis实例A B C D E，两个client 1和2

​        当有一台Redis实例的时钟跳转时:

1. Client 1 acquires lock on nodes A, B, C. Due to a network issue, D and E cannot be reached.
2. The clock on node C jumps forward, causing the lock to expire.
3. Client 2 acquires lock on nodes C, D, E. Due to a network issue, A and B cannot be reached.
4. Clients 1 and 2 now both believe they hold the lock.

​        还有一种类似场景是C在持久化锁到磁盘前故障，然后立即重启。Redlock中已经推荐了延迟重启，但如果时钟突然快进，实例还是会立即重启

​        如果认为时钟突变实际很难发生，也可以看下面进程暂停的例子

1. Client 1 requests lock on nodes A, B, C, D, E.
2. While the responses to client 1 are in flight, client 1 goes into stop-the-world GC.
3. Locks expire on all Redis nodes.
4. Client 2 acquires lock on nodes A, B, C, D, E.
5. Client 1 finishes GC, and receives the responses from Redis nodes indicating that it successfully acquired the lock (they were held in client 1’s kernel network buffers while the process was paused).
6. Clients 1 and 2 now both believe they hold the lock.

​        虽然Redis是用C写的，没有GC，但是客户端是可能遇到GC的，这种情况回到了最初的问题，需要引入fencing token等机制

​        长的网络延迟同样可能造成和进程暂停一样的效果，取决于TCP设置的timeout。如果设置TCP timeout远小于Redis TTL，那么情况可以忽略，但这种情况又需要保证时钟的准确性。

## The synchrony assumptions of Redlock

​        这些例子表面Redlock只适用于假设的同步系统模型(synchronous system model)，即需要满足以下性质

* **bounded network delay**: you can guarantee that packets always arrive within some guaranteed maximum delay
* **bounded process pauses**: in other words, hard real-time constraints, which you typically only find in car airbag systems and suchlike
* **bounded clock error**: cross your fingers that you don’t get your time from a [bad NTP server](http://xenia.media.mit.edu/~nelson/research/ntp-survey99/)

​        synchronous model并不需要同步时钟完全正确，只是需要误差不能超过某一上界。Redlock假设这些延迟、暂停、漂移和锁的TTL相比都很小，如果误差较大，算法就失效了。
