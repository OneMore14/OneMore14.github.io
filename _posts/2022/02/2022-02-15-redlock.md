---
layout: post
title:  "Redlock discussion"
date:   2022-02-15 21:25:01 +0800
categories: "分布式 Redis"
typora-root-url: "../../../"
---

关于Redis实现分布式锁的讨论

# 1. Distributed locks with Redis

*(注: 原文链接[在此](https://redis.io/topics/distlock))*

本文将提出一种新的算法 **Redlock**

#### Safety and Liveness guarantees

在我们看来，高效实现分布式锁至少需要保证以下3个性质

* **Safety property**: Mutual exclusion. At any given moment, only one client can hold a lock.
* **Liveness property A**: Deadlock free. Eventually it is always possible to acquire a lock, even if the client that locked a resource crashes or gets partitioned.
* **Liveness property B**: Fault tolerance. As long as the majority of Redis nodes are up, clients are able to acquire and release locks.

#### Why failover-based implementations are not enough

​        为了理解本文提出的改进，先分析下当前大多数Redis分布式锁的情况。

​        最简单方法是获取锁时创建一个key，且给key设置过期时间，释放锁时删除key。

​        表面上看不错，但有一个问题: 无法应对单点故障。如果使用主从模型，master挂掉后使用slave呢？这仍然不可靠，因为Redis复本更新是异步的

​        在这个模型中有明显的竞争条件:

1. Client A acquires the lock in the master.
2. The master crashes before the write to the key is transmitted to the replica.
3. The replica gets promoted to master.
4. Client B acquires the lock to the same resource A already holds a lock for. **SAFETY VIOLATION!**

​        在某些特殊情况，这种多个客户端能同时拿到锁也许是OK的，那么可以使用上述算法，否则推荐使用下面的算法。

#### Correct implementation with a single instance

​        先看简单的单机情况，加锁时设置

```
 SET resource_name my_random_value NX PX 30000
```

​        NX使只有key不存在时才能成功，PX使key30000毫秒后失效。设置的value必须保证全局唯一(across all clients and all lock requests)

​        一般使用随机的值保证释放锁的安全性，使用以下Lua脚本

```lua
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

​        主要是防止这样一种情况: Client A先加锁，执行了过长的计算过程，导致A设置的key已经过期且B获得锁，当A结束计算准备释放锁时，不能误把B的锁删了*(注: 类似CAS的思想)*。

​        随机值的设定有很多种方法，比如当前时间戳带上client id。目前的算法在单机具有always available的保证下已经安全了，然后会在没有保证的情况下讨论分布式锁。设置的过期时间长度也叫lock validity time或auto release time。

#### The Redlock algorithm

​      在分布式环境下，假设我们有N台Redis master，这些节点完全独立，可能宕机。我们不使用副本机制或其它隐式的协调系统。本文中N = 5，客户端获取锁的算法如下

1. It gets the current time in milliseconds.
2. It tries to acquire the lock in all the N instances sequentially, using the same key name and random value in all the instances. During step 2, when setting the lock in each instance, the client uses a timeout which is small compared to the total lock auto-release time in order to acquire it. For example if the auto-release time is 10 seconds, the timeout could be in the ~ 5-50 milliseconds range. This prevents the client from remaining blocked for a long time trying to talk with a Redis node which is down: if an instance is not available, we should try to talk with the next instance ASAP.
3. The client computes how much time elapsed in order to acquire the lock, by subtracting from the current time the timestamp obtained in step 1. If and only if the client was able to acquire the lock in the majority of the instances (at least 3), and the total time elapsed to acquire the lock is less than lock validity time, the lock is considered to be acquired.
4. If the lock was acquired, its validity time is considered to be the initial validity time minus the time elapsed, as computed in step 3.
5. If the client failed to acquire the lock for some reason (either it was not able to lock N/2+1 instances or the validity time is negative), it will try to unlock all the instances (even the instances it believed it was not able to lock).

*(注: 在单机的基础上，要求获得过半数节点的支持才获取锁成功。且在获取锁的时候，考虑到了访问每个Redis实例共花费的时间，一个优化是在访问每个Redis实例时设置timeout)*

####  Is the algorithm asynchronous?

​        这个算法需要满足一个前提: 当进程间没有同步时钟时，每个进程的本地时间是差不多的，至少时间误差和锁的auto-release time相比很小。这个假设在现实世界中也是基本成立的。此时，我们需要重新更准确定义client持有锁的时间，应当是lock validity time减去为了获得锁花费的时间

#### Retry on failure

​        当客户端无法拿到锁时，其应该在随机延迟后重试，随机是为了防止产生脑裂而没有winner产生。同时，客户端拿到超半数成功的时间越短，产生脑裂的可能就越小，因此理想情况下，客户端应当使用multiplexing同时向Redis实例发消息。

​        同时要强调，当client无法拿到超半数锁时，应当尽快把已经拿到的锁释放。

####  Releasing the lock

​        释放锁就是在所有实例上释放获得的锁

#### Safety arguments

​        首先讨论一个客户端能获取到超半数锁的情况。假设此时所有实例都设置成功，有相同的TTL，但不同实例设置成功的时间并不相同，因此各个实例实际key过期的时间有差异。假设第一个key最晚在T1时间设置成功，最后一个key最晚在T2时间设置成功，则所有实例key都存在的时长是 `MIN_VALIDITY=TTL-(T2-T1)-CLOCK_DRIFT`   

​        当超半数key都设置成功后，如果再来一个client尝试获取分布式锁，一定会获取失败。

​        但我们还想要保证多个client同时获取锁时，不能同时成功。如果一个client获取到了超半数锁，但消耗的时间邻近或超过了maximum validity time，那么client会认为获取锁失败然后释放锁。因此我们只需要考虑耗时小于validity time的情况。根据上文可知，MIN_VALIDITY时间段内，是不可能获取超半数锁的。所以多个客户端同时获取锁的情况只能是一个client获取到锁后发现已经过了TTL，这个client会再释放锁。不过这里的安全性还需要形式化证明。

#### Liveness arguments

​        系统的liveness基于以下3个特点

1. The auto release of the lock (since keys expire): eventually keys are available again to be locked.
2. The fact that clients, usually, will cooperate removing the locks when the lock was not acquired, or when the lock was acquired and the work terminated, making it likely that we don’t have to wait for keys to expire to re-acquire the lock.
3. The fact that when a client needs to retry a lock, it waits a time which is comparably greater than the time needed to acquire the majority of locks, in order to probabilistically make split brain conditions during resource contention unlikely.

#### Performance, crash-recovery and fsync

​        为了保证高性能，可以使用multiplexing同时发请求。

​         同时考虑容错能力，假设我们配置的Redis实例没有任何持久化机制，当前client获取到了5个锁中的3个。然后这3台实例中有一台出错后重启，重启后就丢失了之前的key，此时另一个client可以再获得3个锁，造成不一致。

​        一种办法是开启AOF持久化，但持久化一般是异步写数据到磁盘的，比如每秒一次，那么仍然可能key在被持久化前实例就重启了。如果开启同步持久化，则Redis性能会大打折扣。

​        另一种解决办法是，几乎不用考虑持久化，而是让重启后的实例等待一段时间后再加入分配锁的群体中。等待时间需要超过最大的TTL。不过当超半数机器重启时，会对整个系统带来TTL的延迟。         

#### Making the algorithm more reliable: Extending the lock

​        如果client的任务由几个小步骤组成，可以默认使用较短的 lock validity time，当快到过期时间时，client向所有实例发送一个Lua脚本尝试延长锁的时间。延长锁仍然需要得到超半数支持和在 validity time内完成。延长锁的机制并没有改变算法，因此需要限制重新获取锁的尝试次数，否则会破坏liveness性质



​    



​       



