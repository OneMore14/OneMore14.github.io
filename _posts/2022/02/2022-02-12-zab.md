---
layout: post
title:  "ZooKeeper’s atomic broadcast protocol: Theory and practice"
date:   2022-02-12 22:08:01 +0800
categories: "6.824 分布式"
typora-root-url: "../../../"
---

## 1 Introduction

​        ZooKeeper是一个fault-tolerant distributed coordination service for cloud computing applications。它为其他云计算应用提供分布式协调算法并且维护一个简单的数据库

​		*(注:  highly-available  主要指系统的performance，highly-reliable 指系统的容错性)*   

​        ZooKeeper目标highly-available and highly-reliable，所以多个客户端依靠它实现bootstrapping*(注: 自举? 感觉是指由ZooKeeper完成一些基础工作)*，存储配置信息，存储运行进程的状态，分组管理，实现同步原语和管理错误恢复。ZooKeeper通过replication来实现availability和reliability ，并且设计目标是在以读为主的场景中有优良表现。

​        ZooKeeper数据库由几个host server组成，一般是3或5个，其中一个是leader。只要leader可用，则整个系统可用。Zab是the ZooKeeper Atomic Broadcast algorithm，用于原子更新副本。Zab可以负责集群的leader选择，副本间的同步，管理更新信息以及从宕机中回复到可靠状态。本文将介绍Zab细节。

## 2 Background

​        广播算法(broadcast algorithm)把消息从一个primary进程传递到网络或广播域中的所有其他进程，包括primary进程本身。原子广播协议是一种分布式算法，保证要么正确广播，要么取消广播且没有任何副作用。这是一种在分布式计算的group communication中广泛使用的算法。原子广播也可以被定义为reliable broadcast that satisfies total order。比如，其满足以下性质

* **Validity**: If a correct process broadcasts a message, then all correct processes will eventually deliver it.
* **Uniform Agreement  **: If a process delivers a message, then all correct processes  eventually deliver that message  
* **Uniform Integrity **: For any message m, every process delivers m at most once, and only if m was previously broadcast by the sender of m.  
* **Uniform Total Order **: If processes p and q both deliver messages m and m', then p delivers m before m' if and only if q delivers m before m'.  

有很多种原子广播协议，ZooKeeper采用了Paxos思想，当然，也不是照搬Paxos。

### 2.1 Paxos and design decisions for Zab  

​        Zab有两个重要的要求，要能处理多个客户端操作(outstanding client operations)和从崩溃中快速恢复 。An outstanding transaction is one that has been proposed but not yet delivered. 为了高性能，ZooKeeper需要能处理多个客户端发来的变更状态请求和并发提交的FIFO请求。此外，ZooKeeper需要能在leader崩溃后高效率地恢复。

​        *(注： outstanding client operations中的outstanding应该是指已发出但还未commit的状态)*

​        原版的Paxos协议并不支持multiple outstanding transactions，Paxos不能保证通信内容是FIFO，所以能处理消息丢失和乱序。如果两次变更(transaction)间有顺序依赖，那么Paxos不能保证这种依赖关系。这个问题的解决办法是批处理，将多个变更整合为一个提案(proposal)并且一次只允许一个提案，但这种方案会有性能损耗。

​        The manipulation of the sequence of transactions to use during recovery from primary crashes is claimed to not be efficient enough in Paxos. 为了提高效率，Zab采用 transaction identification 机制保证所有transaction有序。在这种机制下，为了更新新primary process的状态，只需要查询所有进程最高的trans id(transaction identification)，然后从已经接受了该trans的进程中直接复制trans。在Paxos中却不能这么做，所以需要为新primary进程之前还没有"learned a value"(ZooKeeper中术语是"committed a transaction")的序号重新执行Paxos的Phase 1阶段。

​        此外，ZooKeeper还需要满足以下性能要求

* low latency
* good throughput under bursty conditions, handling situations when write workloads increase rapidly  
* smooth failure handling, so that the service can stay up when some non-leader server crashes.  

### 2.2 Crash-recovery system model  

​        ZooKeeper把crash-recovery模型建模为系统模型。系统中有p(1), p(2), ... , p(n)共n个进程，本文中也称为peers，进程间可以互相通信，每个进程都有稳定的存储设备，且各个进程都可能无限崩溃和恢复。一个quorum指所有进程中某个超半数的集合。进程有up和down两个状态，进程从崩溃到开始恢复都是down状态，进程从恢复到下一次崩溃时都是up状态。

​        每个进程对之间都有一个双向通道，通道满足以下性质

* **integrity**    asserting that process p(j) receives a message m from p(i) only if p(i) has sent m
* **prefix**  stating that if process p(j) receives a message m and there is a message m' that precedes m in the sequence of messages p(i) sent to p(j), then p(j) receives m' before m.  

​    为了满足这些特性，ZooKeeper采用TCP，因此有FIFO通道来通信。

### 2.3 Expected properties  

​        为了保证进程间一致性，有一些safety性质需要满足，首先给出一些定义。

​        在崩溃-恢复模型中，如果primary进程*(注: 后文直接写作leader了)*崩溃，则需要选举一个新的leader。由于广播消息是全体有序(total ordered)的，因此要求任意时刻最多只有一个leader，每个leader在位的周期称为epoch。Transaction是指leader广播给其他进程的状态改变消息，以<v, z>表示，v表示新的状态，z代表一个id称为zxid。Transaction首先被leader发起，然后提交(delivered/committed)给消息提交的方法。

​        为了满足一致性，需要保证以下性质

* **Integrity**: If some process delivers <v, z>, then some process has broadcast <v, z>.  
* **Total order**: If some process delivers <v, z> before <v', z'>, then any process that delivers<v', z'> **must** also deliver <v, z> before <v', z'>
* **Agreement**: If some process p(i) delivers <v, z> and some process p(j) delivers <v', z'>,  then either p(i) delivers <v', z'> or p(j) delivers <v, z>.  

​        然后是primary order性质

* **Local primary order**: leader不变时，进程必须按leader广播消息的顺序提交消息
* **Global primary order**: leader有变更时，必须先提交旧leader的消息，再提交新leader的消息
* **Primary integrity**: 如果当前leader广播了消息<v, z>，而一些进程提交了更早leader的消息<v', z'>，那么当前leader在广播<v, z>前也需要提交<v', z'>

## 3 Atomic broadcast protocol  

​        Zab中每个peer有3种状态： 

* following
* leading
* election

​        follower和leader会顺序执行Zab的三个阶段

1. discovery

2. synchronization

3. broadcast

​        在Phase 1前，节点处于选举状态，其执行选举算法，让其他节点为自己投票。在Phase 1阶段开始，节点检查自己到底是follower还是leader。因此，选举阶段也被称为Phase 0.

​        Leader节点和其他follower一起协调，并且在Phase 3阶段最多只能有1个leader，leader负责广播消息。Phase 1和Phase 2重要作用是让各个节点达成一致状态，特别是从崩溃到恢复状态。它们组成了协议中关于恢复的部分，并且保证了trans的顺序。如果没有崩溃发生，所有节点将始终保持Phase 3。在Phase 1，2，3，节点可以决定回到选举阶段，只要有任何错误或超时产生。

​        ZooKeeper客户端是应用程序，通过和至少一个节点通信来使用ZooKeeper服务。客户端提交操作给连接到的节点，如果该操作需要更新状态，Zab层就会广播。如果该操作提交给了follower，那么会转发给leader。如果leader收到了操作请求，它会执行并广播状态变更给follower。读请求可以直接由follower处理。通过发起sync请求，client可以确保连接到的server是最新状态。

​        在Zab中，zxid是实现全体有序的重要组成。transaction<v, z>中的z是<e, c>，其中e是leader发起trans时候的epoch，c是计数用的counter。counter在每一次leader发起新的trans时增长。在新的epoch产生时，e会增大而c会归零。这样每个trans可以根据zxid排序。

​        有四个变量组成每个节点的持久化状态，属于协议中恢复的一部分:

* **history**: a log of transaction proposals accepted
* **acceptedEpoch**: the epoch number of the last NEWEPOCH message accepted  
* **currentEpoch**: the epoch number of the last NEWLEADER message accepted
* **lastZxid**: zxid of the last proposal in the history  

### 3.1 Phases of the protocol  

​        **Phase 0: Leader election** 节点在这个阶段被初始化，进入选举状态。这里没有说必须要用某种特定的选举算法，只要保证绝大概率能选举成功即可。在选举结束后，节点存储它的投票到本地易失性存储中。如果节点p投票给了节点p'，那么p'就是p的prospective leader。只有到phase 3后，如果p'真的收到过半数投票，才会成为established leader。在投票阶段，如果节点投了自己，就进入leading状态，否则是following状态。

​        **Phase 1: Discovery** 这个阶段中，follower和自己的prospective leader通信，这样潜在的leader可以从follower中收集它们已经accept的最新tx。目的是在一个quorum中**discover**出最新被接收tx的序号，然后开启一个新的epoch，让旧leader无法再commit新的提案，完整算法如下



![image-20220213175459619](/assets/2022/02/zab/phase1.png)

​        在阶段开始时，follower会和自己的prospective leader开始leader-follower连接。如果节点p不处于leading状态，而其他节点把p选作prospective leader且尝试leader-follower连接时，p会拒绝连接。被拒绝连接或遇到错误的情况下，会让follower重新回到Phase 0。

​        *(注: Phase 1主要是leader收集信息，根据其follower的状态决定epoch的数值，同时选择具有最新消息的follower的history作为自己的history)*

​        **Phase 2: Synchronization** 同步阶段会包含协议中有关恢复的内容，在集群中同步副本会用到上一阶段leader更新后的history。Leader会与follower通信，从自己的history中propose tx。如果follower自己的tx落后leader，就会确认提案。Leader收到quorum确认后，向其发起一个commit消息。这时，leader就established且不再是prospective，算法2给出完整描述。

![image-20220213185229428](/../../AppData/Roaming/Typora/typora-user-images/image-20220213185229428.png)

​        *(注: 算法2大概就是leader将自己收集到的最新tx分发给follower)*

​        **Phase 3: Broadcast** 如果没有崩溃发生，节点们会永远保持在这个阶段，每当ZooKeeper客户端发起写请求后就广播tx。在一开始，a quorum of peers是一致的，且此时不会有两个leader同时存在。Leader会允许新的follower加入到epoch中，因为a quorum of follower已经足够发起phase 3了。为了追上其他的节点，新来的follower会接受tx广播，并且被包含在leader的已知follower列表中。



​        Phase 3是处理状态改变的唯一阶段，Zab层需要通知ZooKeeper应用它已经准备好状态改变。为此，leader在Phase 3开始时调用ready(e)，使应用可以广播tx。算法3描述如下

![image-20220213190838762](/assets/2022/02/zab/phase3.png)

​        *(注: 算法3中，有新的状态变更时，Leader先广播tx，follower收到后回复ACK，Leader收到足够多ACK后再向这些follower发送commit，follower收到commit后执行真正的commit。但如果当前tx前还有已收到未commit的tx，就会等待)*

​        算法1，2，3显然是异步且不考虑可能的节点崩溃。为了检测崩溃，Zab周期性地在follower和leader间检测心跳。如果leader在一段时间后仍没有收到a quorum of followers的心跳，它会放弃自己的领导权并回到Phase 0. Follower如果一段时间后仍没有收到leader心跳的话，也会重新进入选举状态。

### 3.2 Analytical results  

​        这里简单提一些Zab满足的性质，更深入的证明要看其他论文。下面的invariants可以从上面的3组算法中简单推导，但claims需要使用invariants小心论证 *(注: 具体证明在另外两篇论文中)*



*TODO 大致了解了Zab协议的内容，需要先消化一下和Paxos，Raft做比较，后面的证明和实现之后再继续*
